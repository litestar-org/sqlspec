# SQLSpec Internal Reference Guide

**üéØ PURPOSE**: This file contains implementation details, architectural decisions, and key concepts for generating comprehensive user documentation. Each section can be referenced independently.

**üìù USAGE INSTRUCTIONS FOR AI**:

- Each section has a `[REF-XXX]` tag for easy referencing
- Append new sections at the bottom with incrementing reference numbers
- Include both "what" and "why" for architectural decisions
- Focus on user-facing implications, not just implementation details
- Include code examples that show the user experience

---

## [REF-001] Instrumentation Architecture: Context Managers vs Decorators

**DECISION**: Migrated from decorator-based to context manager-based instrumentation.

**IMPLEMENTATION**:

- **Protocol Layer**: Public methods (`execute`, `execute_many`, `execute_script`) use context managers
- **Driver Layer**: Private methods (`_execute_impl`, `_wrap_select_result`) use context managers
- **Context Managers**: `instrument_operation()` (sync) and `instrument_operation_async()` (async)

**USER BENEFIT**:

- Clean type signatures (no decorator interference)
- Multi-level telemetry (API + driver level)
- Comprehensive tracking of database operations

**CODE EXAMPLES**:

```python
# User calls this
result = driver.execute("SELECT * FROM users")

# Results in telemetry hierarchy:
# 1. High-level: "execute" operation (API usage)
# 2. Low-level: "psycopg_execute" operation (database access)
# 3. Low-level: "psycopg_wrap_select" operation (result processing)
```

**TELEMETRY COVERAGE**:

- OpenTelemetry spans with proper attributes
- Prometheus metrics (counters, histograms, gauges)
- Structured logging with context
- Error tracking and latency monitoring

---

## [REF-002] Psycopg Driver: ModelDTOT and Schema Type Patterns

**DECISION**: Preserve exact `ModelDTOT` and `schema_type` behavior from main branch.

**IMPLEMENTATION**:

- `SelectResult.rows` always contains `dict[str, Any]` objects
- Schema conversion handled by type system and result converter patterns
- `_wrap_select_result` uses conditional return types based on `schema_type` parameter

**USER BENEFIT**:

- Type-safe result conversion with intelligent typing
- Seamless integration with DTO patterns
- Backwards compatibility with existing code

**CODE EXAMPLES**:

```python
# With schema type - gets SelectResult[User]
users = driver.execute("SELECT * FROM users", schema_type=User)

# Without schema type - gets SelectResult[dict[str, Any]]
raw_data = driver.execute("SELECT * FROM users")

# Both work, but typing provides safety
user_name = users.rows[0].name        # ‚úÖ Type-safe
user_name = raw_data.rows[0]["name"]  # ‚úÖ Dict access
```

**OVERLOAD PATTERNS**:

```python
@overload
def execute(statement: SelectBuilder, *, schema_type: type[ModelDTOT]) -> SelectResult[ModelDTOT]: ...

@overload
def execute(statement: SelectBuilder, *, schema_type: None = None) -> SelectResult[dict[str, Any]]: ...
```

---

## [REF-003] Connection Pool Lifecycle and Instrumentation

**DECISION**: Instrument pool operations for observability into connection management.

**IMPLEMENTATION**:

- Pool creation: `_create_pool_impl()` with timing and logging
- Pool closure: `_close_pool_impl()` with cleanup tracking
- Connection provision: Context managers for connection lifecycle
- Session provision: Context managers for driver instances

**USER BENEFIT**:

- Visibility into pool health and performance
- Connection leak detection capabilities
- Pool sizing optimization data

**CONFIG PATTERNS**:

```python
# TypedDict approach for clean configuration
config = PsycopgAsyncConfig(
    pool_config={
        "min_size": 5,
        "max_size": 20,
        "max_lifetime": 3600,
    },
    instrumentation=InstrumentationConfig(
        log_pool_operations=True,
        enable_prometheus=True,
    )
)

# Usage
async with config.provide_session() as driver:
    result = await driver.execute("SELECT 1")
```

**INSTRUMENTATION POINTS**:

- Pool create/destroy operations
- Connection acquire/release timing
- Pool size and utilization metrics
- Connection error rates and types

---

## [REF-004] Clean Code Patterns: Reduced Defensive Programming

**DECISION**: Trust type checker instead of excessive runtime validation.

**IMPLEMENTATION**:

- Removed verbose parameter validation chains
- Simplified error handling patterns
- Trust type hints for parameter contracts
- Focus defensive coding on specific critical paths

**USER BENEFIT**:

- Cleaner, more readable codebase
- Better performance (less runtime checks)
- Clearer error messages when issues occur

**BEFORE/AFTER EXAMPLES**:

```python
# ‚ùå Old: Defensive bloat
if parameters is not None and isinstance(parameters, Sequence):
    for param_set in parameters:
        if isinstance(param_set, dict):
            many_params_list.append(param_set)
        else:
            logger.warning("executemany expects dict, got %s", type(param_set))

# ‚úÖ New: Trust types
if parameters and isinstance(parameters, Sequence):
    final_exec_params = [p for p in parameters if isinstance(p, dict)]
```

**LOGGING PATTERNS**:

```python
# ‚ùå Old: Verbose logging
logger.debug(
    "Executing SQL (Psycopg Sync): %s",
    final_sql,
    extra={
        "dialect": self.dialect,
        "is_many": is_many,
        "is_script": is_script,
        "param_count": len(final_exec_params) if isinstance(final_exec_params, dict) else 0,
    },
)

# ‚úÖ New: Clean logging
logger.debug("Executing SQL: %s", final_sql)
```

---

## [REF-005] Driver Protocol Architecture

**DECISION**: Layered protocol with abstract methods for driver implementations.

**IMPLEMENTATION**:

- **Protocol classes**: Define public API (`execute`, `execute_many`, `execute_script`)
- **Abstract methods**: Driver-specific implementations (`_execute_impl`, `_wrap_*_result`)
- **Instrumentation mixins**: Provide telemetry capabilities
- **Common attributes**: Shared functionality and setup

**USER BENEFIT**:

- Consistent API across all database drivers
- Automatic instrumentation for all drivers
- Type safety through protocol compliance

**INHERITANCE HIERARCHY**:

```
SyncDriverAdapterProtocol
‚îú‚îÄ‚îÄ CommonDriverAttributes (connection management, instrumentation setup)
‚îú‚îÄ‚îÄ SyncInstrumentationMixin (telemetry capabilities)
‚îî‚îÄ‚îÄ Abstract methods for driver implementation

PsycopgSyncDriver
‚îú‚îÄ‚îÄ Inherits from SyncDriverAdapterProtocol
‚îî‚îÄ‚îÄ Implements: _execute_impl, _wrap_select_result, _wrap_execute_result
```

**KEY POINTS FOR DOCS**:

- Users interact with protocol methods, never abstract methods
- All drivers get instrumentation automatically
- Driver implementations focus on database-specific logic
- Protocol handles statement building, type conversion orchestration

---

## [REF-006] Configuration Design: TypedDict Approach

**DECISION**: Use TypedDict for database configuration instead of dataclasses.

**IMPLEMENTATION**:

- `PsycopgConnectionConfig`: Basic connection parameters
- `PsycopgPoolConfig`: Pool-specific configuration (inherits connection params)
- `NotRequired` fields for optional parameters
- Validation happens at runtime, not definition time

**USER BENEFIT**:

- Better IDE support and auto-completion
- Clear documentation of available options
- Type safety without runtime overhead
- Flexible configuration merging

**CONFIG EXAMPLE**:

```python
from sqlspec.adapters.psycopg import PsycopgAsyncConfig

config = PsycopgAsyncConfig(
    pool_config={
        "host": "localhost",
        "port": 5432,
        "user": "myapp",
        "password": "secret",
        "dbname": "production",
        "min_size": 5,
        "max_size": 20,
        "max_lifetime": 3600.0,
    },
    instrumentation=InstrumentationConfig(
        enable_opentelemetry=True,
        enable_prometheus=True,
        service_name="myapp-db",
    )
)
```

---

## [REF-007] Configuration and Driver Protocol Architecture

**DECISION**: Layered architecture with TypedDict configurations and protocol-based driver adapters.

**IMPLEMENTATION**:

### Configuration Hierarchy

- **Base Protocols**: `DatabaseConfigProtocol[ConnectionT, PoolT, DriverT]` defines core interface
- **Sync Configurations**: `SyncDatabaseConfig` (with pooling), `NoPoolSyncConfig` (without pooling)
- **Async Configurations**: `AsyncDatabaseConfig` (with pooling), `NoPoolAsyncConfig` (without pooling)
- **Configuration Registry**: `SQLSpec` class manages configuration instances and provides type-safe access

### Driver Protocol Design

- **Common Base**: `CommonDriverAttributes[ConnectionT, DefaultRowT]` provides shared functionality
- **Instrumentation Mixins**: `SyncInstrumentationMixin` and `AsyncInstrumentationMixin` for telemetry
- **Protocol Classes**: `SyncDriverAdapterProtocol` and `AsyncDriverAdapterProtocol` define public API
- **Abstract Methods**: Driver-specific implementations (`_execute_impl`, `_wrap_select_result`, `_wrap_execute_result`)

**USER BENEFIT**:

- Type-safe configuration management with automatic pool lifecycle
- Consistent API across all database adapters
- Automatic instrumentation and telemetry for all operations
- Clean separation between public API and driver-specific implementation

**CONFIGURATION EXAMPLES**:

```python
# TypedDict configuration approach
config = PsycopgAsyncConfig(
    pool_config={
        "host": "localhost",
        "port": 5432,
        "user": "myapp",
        "dbname": "production",
        "min_size": 5,
        "max_size": 20,
    },
    instrumentation=InstrumentationConfig(
        enable_opentelemetry=True,
        enable_prometheus=True,
    )
)

# Register configuration
sqlspec = SQLSpec()
PsycopgConfig = sqlspec.add_config(config)

# Use through registry
async with sqlspec.provide_session(PsycopgConfig) as driver:
    result = await driver.execute("SELECT * FROM users")
```

**THREE MAIN DRIVER METHODS**:

### 1. `execute()` - Primary Execution Method

**Purpose**: Execute single SQL statements with intelligent return type detection

**Overload Patterns**:

```python
# With schema type - returns SelectResult[User]
users = driver.execute("SELECT * FROM users", schema_type=User)

# Without schema type - returns SelectResult[dict[str, Any]]
raw_data = driver.execute("SELECT * FROM users")

# DML operations - returns ExecuteResult[Any]
result = driver.execute("UPDATE users SET active = true")
```

**Key Features**:

- Automatic detection of SELECT vs DML statements
- Type-safe result conversion based on `schema_type` parameter
- Support for query builders, raw SQL strings, and SQL objects
- Optional statement filters and parameter binding

### 2. `execute_many()` - Batch Operations

**Purpose**: Execute the same statement multiple times with different parameter sets

**Usage Pattern**:

```python
# Batch insert with parameter sequences
parameters = [
    {"name": "Alice", "email": "alice@example.com"},
    {"name": "Bob", "email": "bob@example.com"},
]
result = driver.execute_many(
    "INSERT INTO users (name, email) VALUES (%(name)s, %(email)s)",
    parameters=parameters
)
```

**Return Type**: Always `ExecuteResult[Any]` since batch operations are typically DML

### 3. `execute_script()` - Script Execution

**Purpose**: Execute multi-statement scripts or database-specific commands

**Usage Pattern**:

```python
# Database schema creation script
script_output = driver.execute_script("""
    CREATE TABLE users (id SERIAL PRIMARY KEY, name TEXT);
    CREATE INDEX idx_users_name ON users(name);
""")
```

**Return Type**: Always `str` representing script execution output

**RETURN TYPE ARCHITECTURE**:

### SelectResult[T]

- **Purpose**: Wraps SELECT query results with metadata
- **Generic Type**: `T` represents row type (`dict[str, Any]` or custom model)
- **Key Properties**:
    - `.rows: list[T]` - The actual result rows
    - `.row_count: int` - Number of rows returned
    - Schema conversion handled automatically

### ExecuteResult[T]

- **Purpose**: Wraps DML operation results with execution metadata
- **Key Properties**:
    - `.affected_count: int` - Number of rows affected
    - `.last_insert_id: Optional[Any]` - For INSERT operations
    - Operation-specific metadata

### Raw String (Scripts)

- **Purpose**: Direct output from script execution
- **Contains**: Database response messages, execution logs, error details

**PROTOCOL IMPLEMENTATION FLOW**:

```python
# User calls public method
result = driver.execute("SELECT * FROM users", schema_type=User)

# Protocol method orchestrates:
# 1. Build SQL statement from input
sql_statement = self._build_statement(statement, config, *filters)

# 2. Execute via driver-specific implementation
raw_result = self._execute_impl(sql_statement, parameters, connection)

# 3. Wrap result based on statement type
if CommonDriverAttributes.returns_rows(sql_statement.expression):
    return self._wrap_select_result(sql_statement, raw_result, schema_type=User)
else:
    return self._wrap_execute_result(sql_statement, raw_result)
```

**INSTRUMENTATION INTEGRATION**:

- **Context Managers**: All public methods use `instrument_operation()` for telemetry
- **Multi-Level Tracking**: Protocol-level operations + driver-specific operations
- **Automatic Metrics**: Query counts, latency, error rates, pool status
- **OpenTelemetry**: Distributed tracing with proper span hierarchy
- **Prometheus**: Service-level metrics with custom labels

**KEY POINTS FOR DOCS**:

- Users interact only with the three public methods, never abstract methods
- Return type intelligence based on SQL statement analysis and `schema_type` parameter
- Configuration registry enables type-safe, dependency-injection-style usage
- All database adapters inherit the same API and instrumentation automatically
- TypedDict configurations provide IDE support while maintaining runtime flexibility

---

## [REF-008] SQL Builder System Architecture

**DECISION**: Fluent interface builders with automatic parameter binding and type-safe SQL construction.

**IMPLEMENTATION**:

### Builder Hierarchy

- **Abstract Base**: `QueryBuilder[ResultT]` provides common functionality and parameter management
- **Specific Builders**: `SelectBuilder`, `InsertBuilder`, `UpdateBuilder`, `DeleteBuilder`, `MergeBuilder`
- **Mixin Support**: `WhereClauseMixin` provides common WHERE clause convenience methods
- **Safety Layer**: `SafeQuery` dataclass for validated SQL + parameters before execution

### Core Design Principles

- **Automatic Parameter Binding**: All values are automatically parameterized to prevent SQL injection
- **Fluent Interface**: Method chaining for readable query construction
- **Type Safety**: Generic result types and compile-time validation
- **Dialect Awareness**: SQLGlot-powered SQL generation for multiple database dialects

**USER BENEFIT**:

- **Security**: Automatic parameterization eliminates SQL injection vulnerabilities
- **Readability**: Fluent interface mirrors SQL structure for intuitive query building
- **Type Safety**: Strong typing throughout the build process with intelligent result types
- **Database Portability**: Single API works across multiple database dialects

**BUILDER INTEGRATION WITH DRIVER PROTOCOL**:

The builders integrate seamlessly with the driver protocol - you can pass builder instances directly to `execute()` **without needing to extract parameters manually**:

```python
# Direct builder execution - parameters handled automatically
query = (
    SelectBuilder()
    .select("name", "email", "created_at")
    .from_("users")
    .where("active = true")
    .where_eq("department", "engineering")  # Automatic parameterization
    .order_by("created_at DESC")
    .limit(50)
)

# Execute directly - no manual parameter extraction needed
result = driver.execute(query, schema_type=User)
# Parameters are automatically extracted and bound by the driver
```

**CORE BUILDER PATTERNS**:

### 1. SelectBuilder - Query Construction

**Purpose**: Build type-safe SELECT statements with intelligent result handling

**Key Features**:

```python
# Basic selection with automatic parameter binding
query = (
    SelectBuilder()
    .select("u.name", "u.email", "p.title")
    .from_("users", alias="u")
    .inner_join("profiles p", on="u.id = p.user_id")
    .where_eq("u.active", True)  # Automatically parameterized
    .where_between("u.created_at", start_date, end_date)  # Multiple parameters
    .order_by("u.name")
    .limit(100)
)

# Advanced features
query = (
    SelectBuilder()
    .select("department")
    .count_("*", alias="employee_count")
    .from_("employees")
    .where_in("status", ["active", "on_leave"])  # List automatically parameterized
    .group_by("department")
    .having("COUNT(*) > 5")
)

# Subqueries and CTEs
subquery = SelectBuilder().select("id").from_("active_users").where_eq("verified", True)
main_query = (
    SelectBuilder()
    .select("*")
    .from_("orders")
    .where_exists(subquery)  # Subquery parameters merged automatically
)
```

### 2. InsertBuilder - Data Insertion

**Purpose**: Build safe INSERT statements with flexible value specification

**Key Patterns**:

```python
# Single row insert
insert = (
    InsertBuilder()
    .into("users")
    .columns("name", "email", "department")
    .values("Alice Smith", "alice@company.com", "engineering")
)

# Dictionary-based insert (columns inferred)
insert = (
    InsertBuilder()
    .into("users")
    .values_from_dict({
        "name": "Bob Jones",
        "email": "bob@company.com",
        "department": "marketing"
    })
)

# Bulk insert from data sequence
users_data = [
    {"name": "Carol", "email": "carol@company.com"},
    {"name": "Dave", "email": "dave@company.com"},
]
insert = (
    InsertBuilder()
    .into("users")
    .values_from_dicts(users_data)
)

# INSERT from SELECT
backup_insert = (
    InsertBuilder()
    .into("users_backup")
    .from_select(
        SelectBuilder()
        .select("name", "email", "created_at")
        .from_("users")
        .where("last_login < '2023-01-01'")
    )
)
```

### 3. UpdateBuilder & DeleteBuilder - DML Operations

**Purpose**: Safe modification and deletion with automatic parameter binding

```python
# Update with conditions
update = (
    UpdateBuilder()
    .table("users")
    .set("last_login", datetime.now())
    .set("login_count", "login_count + 1")  # Can use SQL expressions
    .where_eq("email", "user@example.com")
    .where("active = true")
)

# Conditional delete
delete = (
    DeleteBuilder()
    .from_("log_entries")
    .where_between("created_at", start_date, end_date)
    .where_not_in("level", ["ERROR", "CRITICAL"])
)
```

**AUTOMATIC PARAMETER MANAGEMENT**:

### Parameter Binding Strategy

- **Named Parameters**: Builders use dictionary-based parameter storage internally
- **Collision Avoidance**: Auto-generated parameter names (`param_1`, `param_2`, etc.)
- **Merge Logic**: Complex queries automatically merge parameters from subqueries and CTEs
- **Type Preservation**: Parameter values maintain their Python types through to execution

### Security Benefits

```python
# This is SAFE - value is automatically parameterized
user_input = "'; DROP TABLE users; --"
query = SelectBuilder().select("*").from_("users").where_eq("name", user_input)
# Results in: SELECT * FROM users WHERE name = :param_1
# With parameters: {"param_1": "'; DROP TABLE users; --"}
```

**BUILDER-TO-SQL CONVERSION FLOW**:

```python
# 1. Builder constructs SQLGlot expression tree
builder = SelectBuilder().select("name").from_("users").where_eq("active", True)

# 2. build() method produces SafeQuery
safe_query = builder.build()
# safe_query.sql = "SELECT name FROM users WHERE active = :param_1"
# safe_query.parameters = {"param_1": True}

# 3. to_statement() creates SQL object for driver
sql_obj = builder.to_statement(config=SQLConfig())

# 4. Driver protocol accepts builder directly
result = driver.execute(builder, schema_type=User)
# Driver automatically calls to_statement() and extracts parameters
```

**CONVENIENCE METHODS (WhereClauseMixin)**:

The system provides numerous convenience methods for common WHERE conditions:

```python
query = (
    SelectBuilder()
    .select("*")
    .from_("products")
    .where_eq("category", "electronics")        # column = value
    .where_between("price", 100, 500)          # column BETWEEN low AND high
    .where_like("name", "%phone%")             # column LIKE pattern
    .where_in("brand", ["Apple", "Samsung"])   # column IN (values)
    .where_is_not_null("description")          # column IS NOT NULL
    .where_exists(subquery)                    # EXISTS (subquery)
)
```

**INTEGRATION WITH DRIVER EXECUTE METHODS**:

### Direct Builder Execution

```python
# SelectBuilder with driver.execute()
users = await driver.execute(
    SelectBuilder()
    .select("id", "name", "email")
    .from_("users")
    .where_eq("active", True),
    schema_type=User  # Type-safe result conversion
)
# Returns: SelectResult[User]

# InsertBuilder with driver.execute()
result = await driver.execute(
    InsertBuilder()
    .into("users")
    .values_from_dict({"name": "New User", "email": "new@example.com"})
)
# Returns: ExecuteResult[Any]

# No need to pass parameters separately - they're embedded in the builder
```

### Builder with execute_many()

```python
# For batch operations, builders work with parameter sequences
insert_builder = (
    InsertBuilder()
    .into("logs")
    .columns("message", "level", "timestamp")
)

# Execute with multiple parameter sets
result = driver.execute_many(
    insert_builder,
    parameters=[
        {"message": "Info log", "level": "INFO", "timestamp": datetime.now()},
        {"message": "Error log", "level": "ERROR", "timestamp": datetime.now()},
    ]
)
```

**KEY POINTS FOR DOCS**:

- **No Manual Parameter Handling**: Users never need to extract or manage parameters when using builders
- **Type Safety**: Generic result types ensure compile-time validation of operations
- **SQL Injection Prevention**: Automatic parameterization makes SQL injection impossible
- **Fluent Interface**: Method chaining creates readable, maintainable query construction
- **Database Portability**: Single builder API works across PostgreSQL, MySQL, SQLite, BigQuery, etc.
- **Integration Ready**: Builders work directly with all three driver protocol methods (`execute`, `execute_many`, `execute_script`)

---

## [REF-009] SQL Factory: Unified Builder and Expression API

**DECISION**: Single `sql` factory object providing convenient access to all builders and SQL expressions.

**IMPLEMENTATION**:

### Unified Factory Design

- **Single Import**: `from sqlspec import sql` provides access to everything
- **Builder Creation**: `sql.select()`, `sql.insert()`, `sql.update()`, `sql.delete()`, `sql.merge()`
- **Column References**: Dynamic attribute access for columns (e.g., `sql.id`, `sql.name`, `sql.email`)
- **SQL Functions**: Built-in aggregate, string, math, and conversion functions
- **Raw SQL Parsing**: Intelligent detection and parsing of raw SQL strings into builders

### Core Design Philosophy

- **Convenience First**: Single import gives access to all SQL building capabilities
- **Flexible Input**: Accept both traditional builder patterns and raw SQL strings
- **Expression Rich**: Comprehensive set of SQL functions and operators
- **Type Safety**: Maintain strong typing throughout the expression system

**USER BENEFIT**:

- **Developer Experience**: Single `sql` import reduces cognitive load and import complexity
- **Flexibility**: Seamlessly mix raw SQL strings with programmatic builder calls
- **Completeness**: Rich expression API covers most SQL use cases without complex imports
- **Learning Curve**: Intuitive API that mirrors SQL structure and common patterns

**SQL FACTORY USAGE PATTERNS**:

### Traditional Builder Creation

```python
from sqlspec import sql

# Clean, fluent builder creation
query = (
    sql.select(sql.id, sql.name, sql.email)  # Column references via sql.column_name
    .from_("users")
    .where_eq("active", True)
    .order_by("created_at DESC")
    .limit(50)
)

# DML operations
insert = (
    sql.insert("users")
    .columns("name", "email", "department")
    .values("Alice Smith", "alice@company.com", "engineering")
)

update = (
    sql.update("users")
    .set("last_login", sql.now())  # SQL function calls
    .where_eq("id", user_id)
)
```

### Raw SQL Integration

```python
# Parse raw SQL into builders for modification
query = sql.select("SELECT id, name FROM users WHERE active = 1")
enhanced_query = (
    query
    .where_between("created_at", start_date, end_date)  # Add more conditions
    .order_by("name")
    .limit(100)
)

# Mix raw SQL with builder methods
complex_query = (
    sql.select("u.*, p.title")
    .from_("users u")
    .inner_join("profiles p", on="u.id = p.user_id")
    .where("u.department IN ('engineering', 'product')")  # Raw SQL condition
    .where_eq("u.active", True)  # Builder method
)
```

### Dynamic Column References

```python
# Columns accessible as attributes - no string quotes needed
query = (
    sql.select(
        sql.users.id,           # Table-qualified columns
        sql.users.name,
        sql.profiles.title
    )
    .from_("users")
    .inner_join("profiles", on=sql.users.id == sql.profiles.user_id)  # Expression comparison
    .where(sql.users.active == True)
)
```

### Rich Expression API

```python
# Aggregate functions
query = (
    sql.select(
        sql.department,
        sql.count().alias("employee_count"),           # COUNT(*)
        sql.avg(sql.salary).alias("avg_salary"),       # AVG(salary)
        sql.max(sql.created_at).alias("newest_hire")   # MAX(created_at)
    )
    .from_("employees")
    .group_by(sql.department)
    .having(sql.count() > 5)
)

# String and math functions
query = (
    sql.select(
        sql.upper(sql.name).alias("name_upper"),       # UPPER(name)
        sql.concat(sql.first_name, " ", sql.last_name).alias("full_name"),  # CONCAT
        sql.round(sql.salary / 12, 2).alias("monthly_salary")  # ROUND(salary/12, 2)
    )
    .from_("employees")
)
```

### Advanced SQL Functions

```python
# CASE expressions
salary_category = (
    sql.case()
    .when(sql.salary < 50000, "Junior")
    .when(sql.salary < 100000, "Mid-level")
    .when(sql.salary < 150000, "Senior")
    .else_("Executive")
    .end()
)

query = sql.select(sql.name, salary_category.alias("level")).from_("employees")

# Window functions
query = (
    sql.select(
        sql.name,
        sql.salary,
        sql.row_number(
            partition_by=sql.department,
            order_by=sql.salary.desc()
        ).alias("salary_rank")
    )
    .from_("employees")
)

# JSON operations (database-specific)
query = (
    sql.select(
        sql.id,
        sql.json_extract(sql.metadata, "$.tags").alias("tags"),
        sql.json_value(sql.profile, "$.preferences.theme").alias("theme")
    )
    .from_("users")
)
```

### Type Conversion and NULL Handling

```python
query = (
    sql.select(
        sql.id,
        sql.cast(sql.created_at, "DATE").alias("creation_date"),        # CAST conversion
        sql.coalesce(sql.nickname, sql.first_name).alias("display_name"), # NULL handling
        sql.nvl(sql.phone, "No phone provided").alias("contact_phone")    # Oracle-style NVL
    )
    .from_("users")
)
```

**RAW SQL PARSING INTELLIGENCE**:

### Automatic Statement Detection

```python
# Factory automatically detects SQL type and creates appropriate builder
select_builder = sql.select("SELECT * FROM users WHERE active = 1")
insert_builder = sql.insert("INSERT INTO logs (message) VALUES ('System started')")
update_builder = sql.update("UPDATE users SET last_login = NOW() WHERE id = 1")

# Can then enhance with additional builder methods
enhanced = (
    select_builder
    .where_between("created_at", start_date, end_date)
    .order_by("name")
    .limit(50)
)
```

### RETURNING Clause Detection

```python
# Factory detects RETURNING clauses for proper result type handling
returning_insert = sql.insert(
    "INSERT INTO users (name, email) VALUES ('John', 'john@example.com') RETURNING id, created_at"
)
# When executed, will return SelectResult instead of ExecuteResult
result = driver.execute(returning_insert)  # SelectResult[dict[str, Any]]
```

**INTEGRATION WITH DRIVER PROTOCOL**:

### Seamless Builder Integration

```python
# sql factory builders work directly with all driver methods
async with sqlspec.provide_session(MyDatabaseConfig) as driver:
    # Traditional builder
    users = await driver.execute(
        sql.select(sql.id, sql.name, sql.email)
        .from_("users")
        .where_eq("active", True),
        schema_type=User
    )

    # Raw SQL enhanced with builder
    complex_result = await driver.execute(
        sql.select("SELECT u.*, COUNT(o.id) as order_count FROM users u")
        .left_join("orders o", on="u.id = o.user_id")
        .where_between("u.created_at", start_date, end_date)
        .group_by("u.id")
        .having("COUNT(o.id) > 0"),
        schema_type=UserWithOrders
    )

    # Batch operations
    await driver.execute_many(
        sql.insert("audit_log").columns("action", "user_id", "timestamp"),
        [
            {"action": "login", "user_id": 1, "timestamp": datetime.now()},
            {"action": "logout", "user_id": 1, "timestamp": datetime.now()},
        ]
    )
```

### Expression Composition

```python
# Build complex expressions and reuse them
active_users_filter = sql.active == True
recent_filter = sql.created_at > (datetime.now() - timedelta(days=30))

# Compose into queries
new_active_users = (
    sql.select(sql.id, sql.name, sql.email)
    .from_("users")
    .where(active_users_filter)
    .where(recent_filter)
)

# Reuse expressions across different queries
user_count = (
    sql.select(sql.count().alias("total"))
    .from_("users")
    .where(active_users_filter)
    .where(recent_filter)
)
```

**KEY POINTS FOR DOCS**:

- **Single Import Philosophy**: `from sqlspec import sql` gives access to all SQL building capabilities
- **Raw SQL Integration**: Seamlessly parse and enhance existing SQL strings with builder methods
- **Column Attribute Access**: `sql.column_name` provides clean, IDE-friendly column references
- **Rich Expression Library**: Comprehensive functions covering aggregates, strings, math, JSON, and more
- **Type Safety Preservation**: Factory maintains strong typing throughout expression and builder systems
- **Database Agnostic**: Same API works across PostgreSQL, MySQL, SQLite, BigQuery, and other supported databases
- **Builder Enhancement**: Raw SQL can be parsed into builders and then enhanced with additional conditions, joins, etc.
- **Zero Import Complexity**: Reduces cognitive load by providing everything through a single, intuitive interface

---

## [REF-010] AioSQL Integration: File-Based SQL with Full SQLSpec Power

**DECISION**: Complete integration with AioSQL enabling file-based SQL organization while preserving all SQLSpec capabilities.

**IMPLEMENTATION**:

### Integration Architecture

- **Singleton Caching**: `AiosqlLoader` parses SQL files once and caches forever using metaclass
- **Typed Query Objects**: `AiosqlQuery` wraps SQL with type annotations and builder API support
- **Adapter Bridge**: `AiosqlSyncAdapter` and `AiosqlAsyncAdapter` implement aiosql protocol with SQLSpec drivers
- **Service Layer**: `AiosqlService` provides high-level abstractions and advanced configuration
- **Builder API Magic**: Loaded queries support full SQLSpec builder patterns (.where(), .limit(), .order_by())
- **Filter Integration**: Special `_sqlspec_filters` parameter for dynamic filter application

### Core Design Principles

- **Best of Both Worlds**: Combine aiosql's file organization with SQLSpec's power
- **Zero Compromise**: Full compatibility with entire SQLSpec ecosystem
- **Performance First**: Singleton caching and optimized query execution
- **Type Safety**: Full type annotation support with return type inference

**USER BENEFIT**:

- **File Organization**: Organize SQL in files using familiar aiosql conventions
- **Performance**: Singleton caching eliminates re-parsing overhead
- **Builder Integration**: Use SQLSpec builder API on file-loaded queries seamlessly
- **Full Ecosystem**: Access to all SQLSpec features (filters, instrumentation, validation)
- **Migration Path**: Easy migration from existing aiosql setups to SQLSpec power
- **Developer Experience**: Type-safe queries with IDE support and autocompletion

**AIOSQL INTEGRATION PATTERNS**:

### File-Based Query Organization

```sql
# queries.sql - Standard aiosql format
-- name: get_all_users^
SELECT
    id, name, email, department, age, salary, hire_date, active,
    ARRAY['SQL', 'Python', 'Analytics'] as skills
FROM users
WHERE active = TRUE
ORDER BY hire_date DESC

-- name: get_user_by_id$
SELECT * FROM users WHERE id = :user_id AND active = TRUE

-- name: create_user<!
INSERT INTO users (name, email, department, age, salary, hire_date, active)
VALUES (:name, :email, :department, :age, :salary, :hire_date, :active)
RETURNING id, name, email, department

-- name: search_high_performers^
SELECT * FROM users
WHERE salary > :min_salary
  AND active = TRUE
  AND (name ILIKE '%' || :search_term || '%'
       OR email ILIKE '%' || :search_term || '%')
```

### Singleton Caching Power

```python
from sqlspec.extensions.aiosql import AiosqlLoader

# First load - parses file and caches
loader1 = AiosqlLoader("queries.sql", dialect="postgresql")

# Second load - instant retrieval from cache
loader2 = AiosqlLoader("queries.sql", dialect="postgresql")

# loader1 is loader2 -> True (same instance!)
print(f"Cached loading is {first_load_time / cached_load_time:.1f}x faster!")
```

### Typed Query Objects with Builder API Magic

```python
from sqlspec.extensions.aiosql import AiosqlLoader
from sqlspec import sql

# Load with type annotations
loader = AiosqlLoader("queries.sql", dialect="postgresql")
get_users = loader.get_query("get_all_users", return_type=User)

# Use as-is
result = driver.execute(get_users, schema_type=User)

# OR enhance with builder API magic
enhanced_query = (
    get_users
    .where("salary > 75000")  # Add dynamic conditions
    .where_eq("department", "Engineering")  # Builder method chaining
    .order_by("salary DESC", "hire_date ASC")  # Complex ordering
    .limit(10)  # Pagination
)

# Execute enhanced query
top_engineers = driver.execute(enhanced_query, schema_type=User)
```

### Advanced Filter Integration

```python
from sqlspec.statement.filters import SearchFilter, LimitOffsetFilter

# Traditional aiosql execution with SQLSpec filters
loader = AiosqlLoader("queries.sql", dialect="postgresql")

with driver.provide_connection() as conn:
    # Special _sqlspec_filters parameter
    result = queries.get_all_users(
        conn,
        department="Engineering",
        _sqlspec_filters=[
            SearchFilter("name", "John"),      # Text search
            LimitOffsetFilter(10, 0),          # Pagination
        ]
    )
```

### Service Layer for Advanced Workflows

```python
from sqlspec.extensions.aiosql import AiosqlService
from sqlspec.statement.filters import LimitOffsetFilter

# Create service with default configuration
service = AiosqlService(
    driver,
    default_filters=[LimitOffsetFilter(limit=100, offset=0)],  # Auto-pagination
    allow_sqlspec_filters=True  # Enable _sqlspec_filters
)

# Load queries through service (with caching)
queries = service.load_queries("user_queries.sql")

# Execute with service enhancements
result = service.execute_query_with_filters(
    queries.get_users_by_department,
    connection=None,
    parameters={"department": "Engineering"},
    filters=[SearchFilter("email", "@company.com")],
    schema_type=User
)
```

**ADAPTER PATTERN BRIDGE**:

### Native AioSQL Protocol Support

```python
import aiosql
from sqlspec.extensions.aiosql import AiosqlSyncAdapter, AiosqlAsyncAdapter

# Create SQLSpec driver
config = PsycopgAsyncConfig(pool_config={"host": "localhost", "dbname": "myapp"})
driver = config.create_driver()

# Bridge to aiosql
adapter = AiosqlAsyncAdapter(
    driver,
    default_filters=[LimitOffsetFilter(limit=1000)],  # Default pagination
    allow_sqlspec_filters=True  # Enable advanced filtering
)

# Use with standard aiosql
queries = aiosql.from_path("queries.sql", adapter)

# Now all queries have SQLSpec power under the hood!
users = await queries.get_all_users(
    conn,
    department="Engineering",
    _sqlspec_filters=[SearchFilter("name", "John")]  # SQLSpec magic!
)
```

### Seamless Migration from AioSQL

```python
# Before: Standard aiosql
import aiosql
import psycopg2

queries = aiosql.from_path("queries.sql", "psycopg2")
result = queries.get_users(conn, department="Engineering")

# After: AioSQL + SQLSpec (drop-in replacement!)
from sqlspec.extensions.aiosql import AiosqlSyncAdapter
from sqlspec.adapters.psycopg import PsycopgSyncConfig

driver = PsycopgSyncConfig(...).create_driver()
adapter = AiosqlSyncAdapter(driver)
queries = aiosql.from_path("queries.sql", adapter)

# Same interface, but now with SQLSpec power!
result = queries.get_users(
    conn,
    department="Engineering",
    _sqlspec_filters=[LimitOffsetFilter(50, 0)]  # NEW: SQLSpec filters!
)
```

**COMPREHENSIVE QUERY LIFECYCLE**:

### File to Execution Flow

#### 1. File-based SQL organization (aiosql format)

The contents of `analytics.sql`:

```sql
-- name: complex_analytics^
SELECT
    department,
    COUNT(*) as employee_count,
    AVG(salary) as avg_salary,
    MAX(hire_date) as newest_hire
FROM users
WHERE active = TRUE
GROUP BY department
HAVING COUNT(*) > :min_employees
```

```python
# 2. Singleton-cached loading
loader = AiosqlLoader("analytics.sql", dialect="postgresql")
analytics_query = loader.get_query("complex_analytics", return_type=Analytics)

# 3. Builder API enhancement
enhanced_query = (
    analytics_query
    .where("department IN ('Engineering', 'Product')")  # Additional filters
    .order_by("avg_salary DESC")  # Dynamic ordering
    .limit(5)  # Top 5 departments
)

# 4. Advanced filter application
filters = [
    SearchFilter("department", "eng"),  # Text search in department
    LimitOffsetFilter(3, 0),           # Override limit for pagination
]

# 5. Type-safe execution with full ecosystem
result = await driver.execute(
    enhanced_query,
    parameters={"min_employees": 5},
    filters=filters,
    schema_type=Analytics  # Type-safe result conversion
)
```

### Real-World Complex Scenario

```python
# Enterprise data pipeline with file-based queries
from sqlspec.extensions.aiosql import AiosqlLoader, AiosqlService
from sqlspec.statement.filters import SearchFilter, LimitOffsetFilter
from datetime import datetime, timedelta

# Load different query collections
user_loader = AiosqlLoader("user_queries.sql", dialect="postgresql")
analytics_loader = AiosqlLoader("analytics_queries.sql", dialect="postgresql")
reporting_loader = AiosqlLoader("reporting_queries.sql", dialect="postgresql")

# Create service for advanced workflows
service = AiosqlService(
    driver,
    default_filters=[LimitOffsetFilter(limit=1000, offset=0)],
    allow_sqlspec_filters=True
)

# Complex data pipeline
async def generate_department_report(department: str, start_date: datetime):
    # 1. Get users with builder enhancement
    users_query = user_loader.get_query("get_users_by_department", return_type=User)
    users = await driver.execute(
        users_query
        .where("hire_date >= :start_date")
        .where_eq("active", True)
        .order_by("salary DESC"),
        parameters={"department": department, "start_date": start_date},
        schema_type=User
    )

    # 2. Analytics with filters
    analytics_query = analytics_loader.get_query("department_performance", return_type=Analytics)
    performance = await driver.execute(
        analytics_query,
        parameters={"department": department, "period_days": 90},
        filters=[LimitOffsetFilter(10, 0)],
        schema_type=Analytics
    )

    # 3. Complex reporting with service
    reports = service.load_queries("complex_reports.sql")
    revenue_data = await service.execute_query_with_filters(
        reports.department_revenue_analysis,
        connection=None,
        parameters={"department": department},
        filters=[SearchFilter("status", "completed")],
        schema_type=RevenueReport
    )

    return {
        "users": users.rows,
        "performance": performance.rows,
        "revenue": revenue_data
    }
```

**PERFORMANCE CHARACTERISTICS**:

### Optimization Features

```python
# Singleton caching - parse once, use forever
loader1 = AiosqlLoader("huge_file.sql")  # ~100ms first time
loader2 = AiosqlLoader("huge_file.sql")  # ~0.1ms cached (1000x faster!)

# Memory efficiency - shared instances
assert loader1 is loader2  # Same object in memory

# Lazy query enhancement
query = loader.get_query("complex_query")
# Enhancement only happens when executed
enhanced = query.where("active = true").limit(10)  # Fast transformation
```

### Production Metrics

```python
# Performance showcase from real usage
metrics = {
    "File Parsing": "Once per file",          # Singleton eliminates re-parsing
    "Query Loading": "< 1ms (cached)",        # Lightning-fast retrieval
    "Filter Application": "< 0.1ms",          # Efficient SQL transformation
    "Memory Usage": "Minimal",                # Shared cached queries
    "Type Safety": "100%",                    # Full validation support
    "Builder Integration": "Seamless",        # Zero overhead enhancement
}
```

**ECOSYSTEM INTEGRATION SHOWCASE**:

### All SQLSpec Features Work

```python
# File-based queries + Full SQLSpec ecosystem
loader = AiosqlLoader("enterprise_queries.sql", dialect="postgresql")

# 1. Builder API integration
query = (
    loader.get_query("user_analytics")
    .where(sql.salary > 75000)        # Expression trees
    .where_between("age", 25, 45)     # Convenience methods
    .order_by("department", "salary DESC")  # Complex ordering
    .limit(100)                       # Pagination
)

# 2. Advanced filters
filters = [
    SearchFilter("name", "john"),           # Text search
    LimitOffsetFilter(50, 0),              # Pagination
    # CustomFilter(),                     # User-defined filters
]

# 3. Type-safe execution with validation
result = await driver.execute(
    query,
    parameters={"department": "Engineering"},
    filters=filters,
    schema_type=User,  # Pydantic/msgspec validation
    # All driver protocol features work!
)

# 4. Instrumentation and monitoring (automatic)
# - OpenTelemetry tracing
# - Prometheus metrics
# - Query performance tracking
# - Error reporting
```

**KEY POINTS FOR DOCS**:

- **Zero Migration Cost**: Drop-in replacement for existing aiosql setups
- **Performance Revolution**: Singleton caching provides massive speedups
- **Builder API Magic**: Enhance file-loaded queries with SQLSpec builder patterns
- **Filter Ecosystem**: Use powerful SQLSpec filters through `_sqlspec_filters` parameter
- **Type Safety**: Full type annotation support with return type inference
- **Service Abstractions**: High-level service for complex workflows and default configurations
- **Comprehensive Integration**: Works with ALL SQLSpec features (drivers, validation, instrumentation)
- **File Organization**: Maintain clean separation of SQL logic in organized files
- **Developer Experience**: Best-in-class IDE support, autocompletion, and error detection

---

## [REF-011] SQLStatement Architecture: Unified SQL Processing with Pipeline System

**DECISION**: Create a comprehensive SQL processing system with SQLStatement as the core orchestrator, integrating pipelines, validators, analyzers, and transformers.

**IMPLEMENTATION**:

### Core SQLStatement Architecture

- **Immutable Design**: SQLStatement instances are immutable - modification methods return new instances
- **Configuration-Driven**: `SQLConfig` controls parsing, validation, transformation, and analysis behavior
- **Pipeline Integration**: Processing components (validators, analyzers, transformers) run through unified pipeline
- **Parameter Management**: Intelligent parameter binding with multiple style support (named, positional, pyformat)
- **Type Safety**: Strong typing throughout with generic result types and schema conversion

### Pipeline System Architecture

- **ProcessorProtocol**: Common interface for all processing components
- **TransformerPipeline**: Orchestrates execution of multiple processors in sequence
- **Component Types**: Validators (security checks), Analyzers (metadata extraction), Transformers (SQL modification)
- **Aggregated Results**: Pipeline collects validation results and applies transformations

### Configuration and Initialization Flow

```python
# SQLConfig controls all processing behavior
config = SQLConfig(
    enable_parsing=True,           # SQLGlot parsing for transformations
    enable_validation=True,        # Security and safety checks
    enable_transformations=True,   # SQL modification capabilities
    enable_analysis=True,          # Metadata extraction
    strict_mode=True,             # Fail fast on validation errors
    processing_pipeline_components=[...],  # Custom processors
)

# SQLStatement initialization triggers full pipeline
stmt = SQL(
    "SELECT * FROM users WHERE id = :user_id",
    parameters={"user_id": 123},
    config=config
)
```

**USER BENEFIT**:

- **Security First**: Automatic SQL injection prevention and security validation
- **Rich Metadata**: Deep insights into query structure, complexity, and characteristics
- **Performance**: Intelligent caching at multiple levels (parsing, analysis, validation)
- **Developer Experience**: Type-safe, immutable objects with comprehensive introspection
- **Extensibility**: Plugin architecture for custom validators, analyzers, and transformers

**CORE SQLSTATEMENT PATTERNS**:

### 1. Immutable Statement Creation and Modification

```python
from sqlspec.statement.sql import SQL, SQLConfig

# Initial statement creation
base_stmt = SQL(
    "SELECT id, name, email FROM users WHERE active = true",
    config=SQLConfig(enable_analysis=True, enable_validation=True)
)

# Immutable modifications - each returns new instance
filtered_stmt = base_stmt.where("department = 'Engineering'")
paginated_stmt = filtered_stmt.limit(50).offset(0)
ordered_stmt = paginated_stmt.order_by("name ASC", "created_at DESC")

# Original statement unchanged
assert base_stmt.sql != paginated_stmt.sql
assert base_stmt is not paginated_stmt  # Different instances
```

### 2. Configuration-Driven Processing

```python
# Minimal configuration - basic functionality
minimal_config = SQLConfig(
    enable_parsing=False,    # Skip SQLGlot parsing
    enable_validation=False, # Skip security checks
    enable_analysis=False    # Skip metadata extraction
)

# Full-featured configuration
full_config = SQLConfig(
    enable_parsing=True,
    enable_validation=True,
    enable_transformations=True,
    enable_analysis=True,
    strict_mode=True,
    processing_pipeline_components=[
        StatementAnalyzer(cache_size=1000),
        CustomSecurityValidator(),
        QueryOptimizer(),
    ]
)

# Same SQL, different processing based on config
basic_stmt = SQL("SELECT * FROM users", config=minimal_config)
enhanced_stmt = SQL("SELECT * FROM users", config=full_config)

# Enhanced statement has rich metadata
analysis = enhanced_stmt.analyze()
validation = enhanced_stmt.validate()
```

### 3. Parameter Management and Security

```python
# Multiple parameter styles supported
named_stmt = SQL(
    "SELECT * FROM users WHERE id = :user_id AND dept = :department",
    parameters={"user_id": 123, "department": "Engineering"}
)

positional_stmt = SQL(
    "SELECT * FROM users WHERE id = ? AND dept = ?",
    parameters=[123, "Engineering"]
)

# Mixed parameter sources (args + kwargs)
mixed_stmt = SQL(
    "SELECT * FROM users WHERE id = :user_id AND active = ?",
    args=[True],
    kwargs={"user_id": 123}
)

# Automatic parameter conversion for different database drivers
sql_server_sql = stmt.to_sql(placeholder_style="named_at")     # @user_id
postgres_sql = stmt.to_sql(placeholder_style="pyformat_named") # %(user_id)s
mysql_sql = stmt.to_sql(placeholder_style="qmark")            # ?
```

**PIPELINE SYSTEM DEEP DIVE**:

### Component Architecture

```python
from sqlspec.statement.pipelines.base import ProcessorProtocol
from sqlglot import exp

class CustomAnalyzer(ProcessorProtocol[exp.Expression]):
    """Custom processor example."""

    def process(
        self,
        expression: exp.Expression,
        dialect: Optional[DialectType] = None,
        config: Optional[SQLConfig] = None,
    ) -> tuple[exp.Expression, Optional[ValidationResult]]:
        # Analyze the expression
        analysis_result = self._perform_analysis(expression)

        # Return unchanged expression (analyzers don't modify)
        # and optional validation result
        return expression, None

    def _perform_analysis(self, expr: exp.Expression) -> AnalysisResult:
        # Custom analysis logic
        pass
```

### Pipeline Execution Flow

```python
# 1. SQLStatement creation triggers pipeline
stmt = SQL("SELECT * FROM users WHERE id = 1", config=config)

# 2. Pipeline processes expression through all components
pipeline = config.get_pipeline()
transformed_expr, aggregated_validation = pipeline.execute(
    parsed_expression,
    dialect,
    config
)

# 3. Results integrated back into SQLStatement
# - Transformed expression becomes new statement
# - Validation results available via .validation_result
# - Analysis results available via .analyze()
```

### Built-in Pipeline Components

#### Validators (Security & Safety)

```python
from sqlspec.statement.pipelines.validators import (
    InjectionValidator,      # SQL injection detection
    PreventDDL,             # Block dangerous DDL operations
    RiskyDML,               # Flag potentially dangerous DML
    SuspiciousKeywords,     # Detect suspicious SQL patterns
    TautologyConditions,    # Find always-true/false conditions
)

# Default validation pipeline
default_validators = [
    InjectionValidator(),
    RiskyDML(),
    PreventDDL(),
    SuspiciousKeywords(risk_level=RiskLevel.MEDIUM),
]

config = SQLConfig(
    enable_validation=True,
    processing_pipeline_components=default_validators
)
```

#### Analyzers (Metadata Extraction)

```python
from sqlspec.statement.pipelines.analyzers import StatementAnalyzer

# Rich metadata extraction
analyzer = StatementAnalyzer(cache_size=1000)
stmt = SQL(
    """
    SELECT u.name, u.email, COUNT(o.id) as order_count,
           AVG(o.amount) as avg_order_value
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
    WHERE u.department IN ('Sales', 'Marketing')
      AND u.active = true
    GROUP BY u.id, u.name, u.email
    HAVING COUNT(o.id) > 0
    ORDER BY avg_order_value DESC
    LIMIT 50
    """,
    config=SQLConfig(enable_analysis=True)
)

analysis = stmt.analyze()
print(f"Statement Type: {analysis.statement_type}")     # Select
print(f"Tables: {analysis.tables}")                     # ['users', 'orders']
print(f"Complexity Score: {analysis.complexity_score}") # 8 (joins + aggregates + conditions)
print(f"Join Count: {analysis.join_count}")             # 1
print(f"Aggregate Functions: {analysis.aggregate_functions}") # ['count', 'avg']
print(f"Uses Subqueries: {analysis.uses_subqueries}")   # False
```

#### Transformers (SQL Modification)

```python
from sqlspec.statement.pipelines.transformers import (
    AuditCommentAppender,    # Add audit comments
    ParameterizeLiterals,    # Convert literals to parameters
    ForceWhereClause,       # Ensure WHERE clauses exist
    ColumnPruner,           # Remove unnecessary columns
)

# SQL transformation pipeline
transformers = [
    ParameterizeLiterals(),      # Security: parameterize all literals
    ForceWhereClause(),          # Safety: require WHERE clauses
    AuditCommentAppender(user_id="current_user"), # Audit: add tracking
]

config = SQLConfig(
    enable_transformations=True,
    processing_pipeline_components=transformers
)

original_sql = "SELECT * FROM users WHERE active = 1"
stmt = SQL(original_sql, config=config)

# Transformed SQL with audit comment and parameterized literal
transformed = stmt.transform()
print(transformed.sql)
# Output: "SELECT * FROM users WHERE active = :param_1 -- Modified by: current_user"
```

**COMPREHENSIVE ANALYZER ECOSYSTEM**:

### StatementAnalyzer - Real-Time Query Intelligence

```python
from sqlspec.statement.pipelines.analyzers import StatementAnalyzer

# Comprehensive real-time analysis
analyzer = StatementAnalyzer(
    cache_size=1000,                 # Analysis result caching
    max_join_count=10,               # Join complexity thresholds
    max_subquery_depth=3,            # Subquery nesting limits
    max_function_calls=20,           # Function usage limits
    max_where_conditions=15,         # WHERE clause complexity
)

# Real-time analysis capabilities:
analysis = analyzer.analyze_expression(expression)

# Structural Analysis
print(f"Statement Type: {analysis.statement_type}")           # Select, Insert, Update, Delete
print(f"Primary Table: {analysis.table_name}")               # Main table being queried
print(f"All Tables: {analysis.tables}")                      # All referenced tables
print(f"Columns: {analysis.columns}")                        # Column references

# Complexity Analysis
print(f"Complexity Score: {analysis.complexity_score}")      # Overall complexity rating
print(f"Join Count: {analysis.join_count}")                  # Number of joins
print(f"Join Types: {analysis.join_types}")                  # Distribution of join types
print(f"Subquery Count: {len(analysis.tables)}")            # Number of subqueries
print(f"Max Subquery Depth: {analysis.max_subquery_depth}")  # Nesting depth

# Performance Indicators
print(f"Uses Subqueries: {analysis.uses_subqueries}")        # Contains subqueries
print(f"Has Returning: {analysis.has_returning}")            # Has RETURNING clause
print(f"Is Insert From Select: {analysis.is_from_select}")   # INSERT FROM SELECT pattern
print(f"Aggregate Functions: {analysis.aggregate_functions}") # GROUP BY/aggregate usage

# Advanced Metrics
print(f"Function Count: {analysis.function_count}")          # SQL function usage
print(f"Correlated Subqueries: {analysis.correlated_subquery_count}") # Expensive subqueries
print(f"Cartesian Products: {analysis.potential_cartesian_products}")  # Performance risks
print(f"Complexity Warnings: {analysis.complexity_warnings}") # Performance warnings
print(f"Complexity Issues: {analysis.complexity_issues}")     # Performance issues
```

**REAL-TIME PROCESSING WORKFLOWS**:

### 1. Security-First Processing Pipeline

```python
# Real-time security processing for user-submitted queries
security_processor = UnifiedProcessor(
    transformers=[
        CommentRemover(),                    # Remove attack vectors
        ParameterizeLiterals(),             # Prevent injection
    ],
    validators=[
        PreventDDL(risk_level=RiskLevel.HIGH),           # Block DDL
        SuspiciousKeywords(risk_level=RiskLevel.MEDIUM), # Detect threats
        TautologyConditions(risk_level=RiskLevel.HIGH),  # Block logic bombs
        RiskyDML(require_where_clause=True),             # Require WHERE clauses
    ],
    cache_analysis=True,
)

# Real-time security validation
def validate_user_query(sql: str, user_context: dict) -> dict:
    expression = sqlglot.parse_one(sql)

    # Single-pass security processing
    transformed_expr, validation_result = security_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(strict_mode=True)
    )

    # Real-time security assessment
    return {
        "is_safe": validation_result.is_safe,
        "risk_level": validation_result.risk_level.name,
        "security_issues": validation_result.issues,
        "warnings": validation_result.warnings,
        "transformed_sql": transformed_expr.sql(),
        "analysis": security_processor._analysis_cache.get(expression.sql())
    }
```

### 2. Performance Optimization Pipeline

```python
# Real-time performance analysis and optimization
performance_processor = UnifiedProcessor(
    analyzers=[StatementAnalyzer(cache_size=1000)],
    transformers=[
        StarExpander(schema_provider=schema_service),    # Explicit columns
        RemoveUnusedColumns(),                           # Optimize projections
    ],
    validators=[
        ExcessiveJoins(max_joins=8, warn_threshold=5),   # Join complexity
        CartesianProductDetector(),                      # Performance risks
    ],
    cache_analysis=True,
)

# Real-time performance insights
def analyze_query_performance(sql: str) -> dict:
    expression = sqlglot.parse_one(sql)

    # Single-pass performance analysis
    optimized_expr, validation_result = performance_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(enable_transformations=True)
    )

    # Extract performance metrics
    analysis = performance_processor._analysis_cache.get(expression.sql())

    return {
        "complexity_score": analysis.metrics.get("complexity_score", 0),
        "join_analysis": {
            "count": analysis.metrics.get("join_count", 0),
            "types": analysis.metrics.get("join_types", {}),
            "cartesian_risk": analysis.metrics.get("cartesian_risk", 0),
        },
        "subquery_analysis": {
            "count": analysis.metrics.get("subquery_count", 0),
            "max_depth": analysis.metrics.get("max_subquery_depth", 0),
            "correlated_count": analysis.metrics.get("correlated_subquery_count", 0),
        },
        "function_analysis": {
            "total_count": analysis.metrics.get("function_count", 0),
            "expensive_functions": analysis.metrics.get("expensive_functions", 0),
        },
        "optimization_applied": optimized_expr.sql() != expression.sql(),
        "performance_warnings": validation_result.warnings,
        "performance_issues": validation_result.issues,
    }
```

### 3. Compliance and Audit Pipeline

```python
# Real-time compliance and audit processing
compliance_processor = UnifiedProcessor(
    transformers=[
        TracingComment(
            user_id="current_user",
            session_id="session_id",
            application="app_name",
            include_timestamp=True
        ),
        CommentRemover(),                    # Clean SQL for logging
    ],
    validators=[
        PreventDDL(),                        # Compliance: No schema changes
        RiskyDML(require_where_clause=True), # Compliance: Require WHERE
    ],
    analyzers=[StatementAnalyzer()],         # Full analysis for audit
    cache_analysis=True,
)

# Real-time compliance checking
def ensure_compliance(sql: str, user_context: dict) -> dict:
    expression = sqlglot.parse_one(sql)

    # Single-pass compliance processing
    audited_expr, validation_result = compliance_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(strict_mode=False)  # Warnings, not errors
    )

    # Extract audit information
    analysis = compliance_processor._analysis_cache.get(expression.sql())

    return {
        "compliant": validation_result.is_safe,
        "audit_trail": {
            "original_sql": expression.sql(),
            "audited_sql": audited_expr.sql(),
            "user": user_context.get("user_id"),
            "timestamp": datetime.now().isoformat(),
            "analysis": {
                "tables_accessed": analysis.metrics.get("tables", []),
                "operation_type": analysis.metrics.get("statement_type"),
                "complexity_score": analysis.metrics.get("complexity_score", 0),
            }
        },
        "compliance_issues": validation_result.issues,
        "compliance_warnings": validation_result.warnings,
    }
```

**REAL-TIME INTEGRATION PATTERNS**:

### 1. Driver Protocol Integration

```python
# Unified processor integrates seamlessly with driver protocol
async with sqlspec.provide_session(DatabaseConfig) as driver:

    # Create processor for session
    session_processor = UnifiedProcessor(
        transformers=[CommentRemover(), ParameterizeLiterals()],
        validators=[PreventDDL(), SuspiciousKeywords()],
        cache_analysis=True,
    )

    # Process query before execution
    expression = sqlglot.parse_one(user_sql)
    safe_expr, validation = session_processor.process(expression)

    if validation.is_safe:
        # Execute processed query
        result = await driver.execute(safe_expr, schema_type=User)
    else:
        # Handle security issues
        raise SecurityError(f"Query blocked: {validation.issues}")
```

### 2. Real-Time API Integration

```python
from fastapi import FastAPI, HTTPException
from sqlspec.statement.pipelines import UnifiedProcessor

app = FastAPI()

# Global processor for API endpoints
api_processor = UnifiedProcessor(
    transformers=[CommentRemover(), TracingComment()],
    validators=[PreventDDL(), SuspiciousKeywords(), RiskyDML()],
    cache_analysis=True,
)

@app.post("/api/query")
async def execute_query(query_request: QueryRequest):
    """Real-time query processing API endpoint."""

    # Parse and process in real-time
    expression = sqlglot.parse_one(query_request.sql)
    processed_expr, validation = api_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(strict_mode=True)
    )

    # Real-time security check
    if not validation.is_safe:
        raise HTTPException(
            status_code=400,
            detail={
                "error": "Query validation failed",
                "issues": validation.issues,
                "risk_level": validation.risk_level.name
            }
        )

    # Extract real-time analysis
    analysis = api_processor._analysis_cache.get(expression.sql())

    # Execute with driver
    async with driver_session() as driver:
        result = await driver.execute(processed_expr)

        return {
            "data": result.rows,
            "metadata": {
                "row_count": result.row_count,
                "analysis": {
                    "complexity_score": analysis.metrics.get("complexity_score", 0),
                    "tables_accessed": analysis.metrics.get("tables", []),
                    "join_count": analysis.metrics.get("join_count", 0),
                }
            }
        }
```

### 3. Middleware Integration

```python
class SQLProcessingMiddleware:
    """Real-time SQL processing middleware."""

    def __init__(self):
        self.processor = UnifiedProcessor(
            transformers=[CommentRemover(), ParameterizeLiterals()],
            validators=[PreventDDL(), SuspiciousKeywords(), TautologyConditions()],
            analyzers=[StatementAnalyzer()],
            cache_analysis=True,
        )

    async def process_request(self, sql: str, context: dict) -> dict:
        """Process SQL request in real-time."""

        # Real-time processing
        expression = sqlglot.parse_one(sql)
        processed_expr, validation = self.processor.process(
            expression,
            dialect=context.get("dialect", "postgresql"),
            config=SQLConfig(strict_mode=context.get("strict_mode", True))
        )

        # Real-time analysis extraction
        analysis = self.processor._analysis_cache.get(expression.sql())

        return {
            "processed_sql": processed_expr.sql(),
            "is_safe": validation.is_safe,
            "security_issues": validation.issues,
            "warnings": validation.warnings,
            "real_time_analysis": {
                "complexity_score": analysis.metrics.get("complexity_score", 0),
                "security_risk": validation.risk_level.name,
                "performance_indicators": {
                    "join_count": analysis.metrics.get("join_count", 0),
                    "subquery_count": analysis.metrics.get("subquery_count", 0),
                    "function_count": analysis.metrics.get("function_count", 0),
                },
                "structural_analysis": {
                    "statement_type": analysis.metrics.get("statement_type"),
                    "tables_accessed": analysis.metrics.get("tables", []),
                    "has_aggregates": analysis.metrics.get("has_aggregates", False),
                    "has_subqueries": analysis.metrics.get("has_subqueries", False),
                }
            }
        }
```

**REAL-TIME MONITORING AND OBSERVABILITY**:

### Pipeline Metrics and Insights

```python
# Real-time pipeline monitoring
class PipelineMonitor:
    """Monitor unified processor performance and insights."""

    def __init__(self, processor: UnifiedProcessor):
        self.processor = processor
        self.metrics = {
            "queries_processed": 0,
            "security_blocks": 0,
            "performance_warnings": 0,
            "cache_hits": 0,
            "processing_times": [],
        }

    def get_real_time_stats(self) -> dict:
        """Get real-time processing statistics."""

        cache_size = len(self.processor._analysis_cache)

        return {
            "pipeline_health": {
                "queries_processed": self.metrics["queries_processed"],
                "cache_efficiency": {
                    "size": cache_size,
                    "hit_ratio": self.metrics["cache_hits"] / max(self.metrics["queries_processed"], 1),
                },
                "security_metrics": {
                    "blocks": self.metrics["security_blocks"],
                    "block_rate": self.metrics["security_blocks"] / max(self.metrics["queries_processed"], 1),
                },
                "performance_metrics": {
                    "warnings": self.metrics["performance_warnings"],
                    "avg_processing_time": sum(self.metrics["processing_times"]) / max(len(self.metrics["processing_times"]), 1),
                }
            },
            "real_time_insights": {
                "most_complex_queries": self._get_complex_queries(),
                "common_security_issues": self._get_security_patterns(),
                "performance_bottlenecks": self._get_performance_issues(),
            }
        }
```

**KEY POINTS FOR DOCS**:

- **Real-Time Processing**: Single-pass analysis, transformation, and validation with minimal overhead
- **Comprehensive Security**: Multi-layered threat detection including injection, DDL prevention, and suspicious patterns
- **Performance Intelligence**: Real-time complexity analysis, join optimization, and cartesian product detection
- **Audit and Compliance**: Automatic audit trails, compliance checking, and regulatory requirement enforcement
- **Shared Analysis**: Unified analysis engine eliminates redundant processing across all components
- **Extensible Architecture**: ProcessorProtocol enables custom validators, transformers, and analyzers
- **Production Ready**: Intelligent caching, minimal overhead, and comprehensive error handling
- **Developer Experience**: Rich real-time insights, type-safe interfaces, and comprehensive observability
- **Integration Friendly**: Seamless integration with driver protocol, APIs, middleware, and monitoring systems
- **Zero-Compromise Security**: Real-time threat detection without sacrificing performance or functionality

---

## [REF-012] Unified Pipeline Architecture: Real-Time SQL Processing and Analysis

**DECISION**: Implement a unified processing pipeline that combines analysis, transformation, and validation in a single pass to eliminate redundant parsing while providing comprehensive real-time SQL insights.

**IMPLEMENTATION**:

### Core Architecture Components

- **UnifiedProcessor**: Central orchestrator that performs analysis once and shares results across all components
- **ProcessorProtocol**: Common interface enabling consistent extension points for all processing components
- **Comprehensive Analysis Engine**: Built-in analysis covering structural patterns, complexity metrics, and security risks
- **Enhanced Component Interfaces**: Optional `validate_with_analysis` and `transform_with_analysis` methods for shared analysis
- **Multi-Level Caching**: Analysis results cached at expression, statement, and component levels

### Real-Time Processing Philosophy

- **Parse Once, Process Once**: Single SQLglot parsing pass eliminates redundant expression tree creation
- **Shared Analysis Results**: Common metrics computed once and distributed to all components
- **Minimal Overhead**: Lightweight processing with intelligent caching for production workloads
- **Live Insights**: Real-time extraction of query complexity, security risks, and structural patterns

**USER BENEFIT**:

- **Real-Time Security**: Immediate detection of SQL injection attempts, suspicious patterns, and risky operations
- **Live Query Intelligence**: Instant insights into query complexity, performance characteristics, and structural analysis
- **Zero-Latency Validation**: Comprehensive security and safety checks with minimal processing overhead
- **Dynamic Transformation**: Real-time SQL modification for security, optimization, and compliance
- **Comprehensive Observability**: Rich metadata extraction for monitoring, debugging, and optimization

**UNIFIED PROCESSOR ARCHITECTURE**:

### Core UnifiedProcessor Design

```python
from sqlspec.statement.pipelines import UnifiedProcessor
from sqlspec.statement.pipelines.validators import (
    ExcessiveJoins, CartesianProductDetector, PreventDDL,
    SuspiciousKeywords, TautologyConditions, RiskyDML
)
from sqlspec.statement.pipelines.transformers import (
    CommentRemover, HintRemover, ParameterizeLiterals,
    TracingComment, StarExpander
)
from sqlspec.statement.pipelines.analyzers import StatementAnalyzer

# Create unified processor with comprehensive pipeline
processor = UnifiedProcessor(
    analyzers=[StatementAnalyzer(cache_size=1000)],
    transformers=[
        CommentRemover(),           # Security: Remove comment-based attacks
        ParameterizeLiterals(),     # Security: Parameterize all literals
        TracingComment(user_id="current_user"),  # Audit: Add execution tracking
    ],
    validators=[
        PreventDDL(),              # Security: Block dangerous DDL operations
        RiskyDML(),                # Security: Flag dangerous DML patterns
        SuspiciousKeywords(),      # Security: Detect suspicious SQL patterns
        TautologyConditions(),     # Security: Find always-true/false conditions
        ExcessiveJoins(),          # Performance: Detect complex join patterns
        CartesianProductDetector(), # Performance: Prevent cartesian products
    ],
    cache_analysis=True,           # Performance: Cache analysis results
)

# Single processing pass with comprehensive analysis
expression = sqlglot.parse_one(sql)
transformed_expr, validation_result = processor.process(
    expression,
    dialect="postgresql",
    config=config
)
```

### Real-Time Analysis Engine

The unified processor performs comprehensive analysis in a single pass:

```python
# Automatic analysis extraction during processing
analysis_result = processor._analysis_cache.get(expression.sql())

# Rich real-time metrics available immediately
metrics = {
    "statement_type": analysis_result.metrics["statement_type"],        # Select, Insert, Update, etc.
    "complexity_score": analysis_result.metrics["complexity_score"],    # Overall complexity rating
    "join_count": analysis_result.metrics["join_count"],                # Number of joins
    "join_types": analysis_result.metrics["join_types"],                # Types of joins used
    "subquery_count": analysis_result.metrics["subquery_count"],        # Number of subqueries
    "max_subquery_depth": analysis_result.metrics["max_subquery_depth"], # Nesting depth
    "function_count": analysis_result.metrics["function_count"],         # SQL functions used
    "table_count": analysis_result.metrics["table_count"],              # Tables referenced
    "cartesian_risk": analysis_result.metrics["cartesian_risk"],        # Cartesian product risk
    "has_aggregates": analysis_result.metrics["has_aggregates"],        # Uses GROUP BY/aggregates
    "has_subqueries": analysis_result.metrics["has_subqueries"],        # Contains subqueries
    "has_window_functions": analysis_result.metrics["has_window_functions"], # Uses window functions
}
```

**COMPREHENSIVE VALIDATOR ECOSYSTEM**:

### 1. Security Validators - Real-Time Threat Detection

#### PreventDDL - Database Schema Protection

```python
from sqlspec.statement.pipelines.validators import PreventDDL

# Blocks dangerous DDL operations in real-time
ddl_validator = PreventDDL(
    risk_level=RiskLevel.HIGH,
    allowed_operations=["CREATE INDEX"],  # Optional whitelist
)

# Detects and blocks:
# - DROP TABLE/DATABASE operations
# - ALTER TABLE structure changes
# - TRUNCATE operations
# - CREATE/DROP USER operations
# - GRANT/REVOKE permission changes
```

#### SuspiciousKeywords - Pattern-Based Threat Detection

```python
from sqlspec.statement.pipelines.validators import SuspiciousKeywords

# Real-time detection of suspicious SQL patterns
keyword_validator = SuspiciousKeywords(
    risk_level=RiskLevel.MEDIUM,
    custom_keywords=["xp_cmdshell", "sp_configure"],  # Custom threat patterns
)

# Detects suspicious patterns:
# - File system operations (LOAD_FILE, INTO OUTFILE)
# - System command execution attempts
# - Information schema probing
# - Time-based attack patterns
# - Union-based injection attempts
```

#### TautologyConditions - Logic Bomb Detection

```python
from sqlspec.statement.pipelines.validators import TautologyConditions

# Detects always-true/false conditions (common in SQL injection)
tautology_validator = TautologyConditions(
    risk_level=RiskLevel.HIGH,
    check_numeric_tautologies=True,   # 1=1, 0=0 patterns
    check_string_tautologies=True,    # 'a'='a' patterns
)

# Real-time detection of:
# - Classic injection patterns (1=1, 1=0)
# - String-based tautologies ('x'='x')
# - Mathematical tautologies (2+2=4)
# - Boolean logic bypasses (true OR false)
```

#### RiskyDML - Dangerous Operation Detection

```python
from sqlspec.statement.pipelines.validators import RiskyDML

# Flags potentially dangerous DML operations
risky_dml_validator = RiskyDML(
    risk_level=RiskLevel.MEDIUM,
    require_where_clause=True,        # Require WHERE in UPDATE/DELETE
    max_affected_threshold=1000,      # Limit bulk operations
)

# Detects risky patterns:
# - UPDATE/DELETE without WHERE clauses
# - Bulk operations exceeding thresholds
# - Cross-table UPDATE operations
# - Subquery-based modifications
```

### 2. Performance Validators - Real-Time Optimization

#### ExcessiveJoins - Join Complexity Analysis

```python
from sqlspec.statement.pipelines.validators import ExcessiveJoins

# Real-time join complexity analysis
join_validator = ExcessiveJoins(
    max_joins=10,                     # Maximum allowed joins
    warn_threshold=7,                 # Warning threshold
    risk_level=RiskLevel.MEDIUM,
)

# Enhanced analysis with shared metrics:
def validate_with_analysis(self, expression, analysis, dialect, config):
    join_count = analysis.metrics.get("join_count", 0)
    join_types = analysis.metrics.get("join_types", {})
    cartesian_risk = analysis.metrics.get("cartesian_risk", 0)

    # Real-time join pattern analysis
    # - Detects excessive join complexity
    # - Identifies join type distribution
    # - Flags potential performance issues
```

#### CartesianProductDetector - Performance Risk Analysis

```python
from sqlspec.statement.pipelines.validators import CartesianProductDetector

# Real-time cartesian product detection
cartesian_validator = CartesianProductDetector(
    risk_level=RiskLevel.HIGH,
    allow_explicit_cross_joins=True,  # Allow intentional CROSS JOINs
    max_table_product_size=1000000,   # Size threshold
)

# Enhanced analysis capabilities:
def validate_with_analysis(self, expression, analysis, dialect, config):
    cross_joins = analysis.metrics.get("cross_joins", 0)
    joins_without_conditions = analysis.metrics.get("joins_without_conditions", 0)

    # Real-time detection of:
    # - Explicit CROSS JOIN operations
    # - Joins without proper conditions
    # - Multiple table FROM clauses without correlation
    # - Potential result set explosion patterns
```

**COMPREHENSIVE TRANSFORMER ECOSYSTEM**:

### 1. Security Transformers - Real-Time SQL Hardening

#### CommentRemover - Attack Vector Elimination

```python
from sqlspec.statement.pipelines.transformers import CommentRemover

# Real-time comment removal for security
comment_remover = CommentRemover(
    enabled=True,
    preserve_mysql_version_comments=False,  # Remove MySQL version comments
)

# Removes security risks:
# - SQL injection via comments
# - Information disclosure in comments
# - Comment-based attack vectors
# - Preserves hints and functional comments
```

#### ParameterizeLiterals - Injection Prevention

```python
from sqlspec.statement.pipelines.transformers import ParameterizeLiterals

# Automatic literal parameterization for security
parameterizer = ParameterizeLiterals(
    enabled=True,
    preserve_numeric_literals=False,  # Parameterize all literals
    parameter_prefix="param_",        # Custom parameter naming
)

# Real-time transformation:
# Original: "SELECT * FROM users WHERE id = 123 AND name = 'John'"
# Transformed: "SELECT * FROM users WHERE id = :param_1 AND name = :param_2"
# Parameters: {"param_1": 123, "param_2": "John"}
```

#### HintRemover - Query Optimization Control

```python
from sqlspec.statement.pipelines.transformers import HintRemover

# Remove database-specific hints for portability
hint_remover = HintRemover()

# Removes optimizer hints:
# - Oracle hints (/*+ INDEX(table_name) */)
# - MySQL hints (/*! SQL_NO_CACHE */)
# - SQL Server hints (WITH (NOLOCK))
# - Preserves functional SQL structure
```

### 2. Audit and Compliance Transformers

#### TracingComment - Execution Tracking

```python
from sqlspec.statement.pipelines.transformers import TracingComment

# Add audit trails to SQL statements
tracing_transformer = TracingComment(
    user_id="current_user",
    session_id="session_123",
    application="myapp",
    include_timestamp=True,
)

# Real-time audit enhancement:
# Original: "SELECT * FROM users WHERE active = true"
# Enhanced: "SELECT * FROM users WHERE active = true -- Executed by: current_user, Session: session_123, App: myapp, Time: 2024-01-15T10:30:00Z"
```

#### StarExpander - Column Visibility Control

```python
from sqlspec.statement.pipelines.transformers import StarExpander

# Expand SELECT * for explicit column control
star_expander = StarExpander(
    schema_provider=schema_service,   # Schema information source
    exclude_sensitive_columns=True,  # Skip PII columns
)

# Real-time expansion:
# Original: "SELECT * FROM users"
# Expanded: "SELECT id, name, email, created_at FROM users"
# (excludes sensitive columns like ssn, password_hash)
```

### 3. Optimization Transformers

#### RemoveUnusedColumns - Query Optimization

```python
from sqlspec.statement.pipelines.transformers import RemoveUnusedColumns

# Remove unused columns for performance
column_optimizer = RemoveUnusedColumns(
    analyze_usage=True,              # Analyze column usage patterns
    preserve_primary_keys=True,      # Keep primary keys
)

# Real-time optimization:
# - Removes unused SELECT columns
# - Optimizes JOIN conditions
# - Reduces data transfer overhead
```

**COMPREHENSIVE ANALYZER ECOSYSTEM**:

### StatementAnalyzer - Real-Time Query Intelligence

```python
from sqlspec.statement.pipelines.analyzers import StatementAnalyzer

# Comprehensive real-time analysis
analyzer = StatementAnalyzer(
    cache_size=1000,                 # Analysis result caching
    max_join_count=10,               # Join complexity thresholds
    max_subquery_depth=3,            # Subquery nesting limits
    max_function_calls=20,           # Function usage limits
    max_where_conditions=15,         # WHERE clause complexity
)

# Real-time analysis capabilities:
analysis = analyzer.analyze_expression(expression)

# Structural Analysis
print(f"Statement Type: {analysis.statement_type}")           # Select, Insert, Update, Delete
print(f"Primary Table: {analysis.table_name}")               # Main table being queried
print(f"All Tables: {analysis.tables}")                      # All referenced tables
print(f"Columns: {analysis.columns}")                        # Column references

# Complexity Analysis
print(f"Complexity Score: {analysis.complexity_score}")      # Overall complexity rating
print(f"Join Count: {analysis.join_count}")                  # Number of joins
print(f"Join Types: {analysis.join_types}")                  # Distribution of join types
print(f"Subquery Count: {len(analysis.tables)}")            # Number of subqueries
print(f"Max Subquery Depth: {analysis.max_subquery_depth}")  # Nesting depth

# Performance Indicators
print(f"Uses Subqueries: {analysis.uses_subqueries}")        # Contains subqueries
print(f"Has Returning: {analysis.has_returning}")            # Has RETURNING clause
print(f"Is Insert From Select: {analysis.is_from_select}")   # INSERT FROM SELECT pattern
print(f"Aggregate Functions: {analysis.aggregate_functions}") # GROUP BY/aggregate usage

# Advanced Metrics
print(f"Function Count: {analysis.function_count}")          # SQL function usage
print(f"Correlated Subqueries: {analysis.correlated_subquery_count}") # Expensive subqueries
print(f"Cartesian Products: {analysis.potential_cartesian_products}")  # Performance risks
print(f"Complexity Warnings: {analysis.complexity_warnings}") # Performance warnings
print(f"Complexity Issues: {analysis.complexity_issues}")     # Performance issues
```

**REAL-TIME PROCESSING WORKFLOWS**:

### 1. Security-First Processing Pipeline

```python
# Real-time security processing for user-submitted queries
security_processor = UnifiedProcessor(
    transformers=[
        CommentRemover(),                    # Remove attack vectors
        ParameterizeLiterals(),             # Prevent injection
    ],
    validators=[
        PreventDDL(risk_level=RiskLevel.HIGH),           # Block DDL
        SuspiciousKeywords(risk_level=RiskLevel.MEDIUM), # Detect threats
        TautologyConditions(risk_level=RiskLevel.HIGH),  # Block logic bombs
        RiskyDML(require_where_clause=True),             # Require WHERE clauses
    ],
    cache_analysis=True,
)

# Real-time security validation
def validate_user_query(sql: str, user_context: dict) -> dict:
    expression = sqlglot.parse_one(sql)

    # Single-pass security processing
    transformed_expr, validation_result = security_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(strict_mode=True)
    )

    # Real-time security assessment
    return {
        "is_safe": validation_result.is_safe,
        "risk_level": validation_result.risk_level.name,
        "security_issues": validation_result.issues,
        "warnings": validation_result.warnings,
        "transformed_sql": transformed_expr.sql(),
        "analysis": security_processor._analysis_cache.get(expression.sql())
    }
```

### 2. Performance Optimization Pipeline

```python
# Real-time performance analysis and optimization
performance_processor = UnifiedProcessor(
    analyzers=[StatementAnalyzer(cache_size=1000)],
    transformers=[
        StarExpander(schema_provider=schema_service),    # Explicit columns
        RemoveUnusedColumns(),                           # Optimize projections
    ],
    validators=[
        ExcessiveJoins(max_joins=8, warn_threshold=5),   # Join complexity
        CartesianProductDetector(),                      # Performance risks
    ],
    cache_analysis=True,
)

# Real-time performance insights
def analyze_query_performance(sql: str) -> dict:
    expression = sqlglot.parse_one(sql)

    # Single-pass performance analysis
    optimized_expr, validation_result = performance_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(enable_transformations=True)
    )

    # Extract performance metrics
    analysis = performance_processor._analysis_cache.get(expression.sql())

    return {
        "complexity_score": analysis.metrics.get("complexity_score", 0),
        "join_analysis": {
            "count": analysis.metrics.get("join_count", 0),
            "types": analysis.metrics.get("join_types", {}),
            "cartesian_risk": analysis.metrics.get("cartesian_risk", 0),
        },
        "subquery_analysis": {
            "count": analysis.metrics.get("subquery_count", 0),
            "max_depth": analysis.metrics.get("max_subquery_depth", 0),
            "correlated_count": analysis.metrics.get("correlated_subquery_count", 0),
        },
        "function_analysis": {
            "total_count": analysis.metrics.get("function_count", 0),
            "expensive_functions": analysis.metrics.get("expensive_functions", 0),
        },
        "optimization_applied": optimized_expr.sql() != expression.sql(),
        "performance_warnings": validation_result.warnings,
        "performance_issues": validation_result.issues,
    }
```

### 3. Compliance and Audit Pipeline

```python
# Real-time compliance and audit processing
compliance_processor = UnifiedProcessor(
    transformers=[
        TracingComment(
            user_id="current_user",
            session_id="session_id",
            application="app_name",
            include_timestamp=True
        ),
        CommentRemover(),                    # Clean SQL for logging
    ],
    validators=[
        PreventDDL(),                        # Compliance: No schema changes
        RiskyDML(require_where_clause=True), # Compliance: Require WHERE
    ],
    analyzers=[StatementAnalyzer()],         # Full analysis for audit
    cache_analysis=True,
)

# Real-time compliance checking
def ensure_compliance(sql: str, user_context: dict) -> dict:
    expression = sqlglot.parse_one(sql)

    # Single-pass compliance processing
    audited_expr, validation_result = compliance_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(strict_mode=False)  # Warnings, not errors
    )

    # Extract audit information
    analysis = compliance_processor._analysis_cache.get(expression.sql())

    return {
        "compliant": validation_result.is_safe,
        "audit_trail": {
            "original_sql": expression.sql(),
            "audited_sql": audited_expr.sql(),
            "user": user_context.get("user_id"),
            "timestamp": datetime.now().isoformat(),
            "analysis": {
                "tables_accessed": analysis.metrics.get("tables", []),
                "operation_type": analysis.metrics.get("statement_type"),
                "complexity_score": analysis.metrics.get("complexity_score", 0),
            }
        },
        "compliance_issues": validation_result.issues,
        "compliance_warnings": validation_result.warnings,
    }
```

**REAL-TIME INTEGRATION PATTERNS**:

### 1. Driver Protocol Integration

```python
# Unified processor integrates seamlessly with driver protocol
async with sqlspec.provide_session(DatabaseConfig) as driver:

    # Create processor for session
    session_processor = UnifiedProcessor(
        transformers=[CommentRemover(), ParameterizeLiterals()],
        validators=[PreventDDL(), SuspiciousKeywords()],
        cache_analysis=True,
    )

    # Process query before execution
    expression = sqlglot.parse_one(user_sql)
    safe_expr, validation = session_processor.process(expression)

    if validation.is_safe:
        # Execute processed query
        result = await driver.execute(safe_expr, schema_type=User)
    else:
        # Handle security issues
        raise SecurityError(f"Query blocked: {validation.issues}")
```

### 2. Real-Time API Integration

```python
from fastapi import FastAPI, HTTPException
from sqlspec.statement.pipelines import UnifiedProcessor

app = FastAPI()

# Global processor for API endpoints
api_processor = UnifiedProcessor(
    transformers=[CommentRemover(), TracingComment()],
    validators=[PreventDDL(), SuspiciousKeywords(), RiskyDML()],
    cache_analysis=True,
)

@app.post("/api/query")
async def execute_query(query_request: QueryRequest):
    """Real-time query processing API endpoint."""

    # Parse and process in real-time
    expression = sqlglot.parse_one(query_request.sql)
    processed_expr, validation = api_processor.process(
        expression,
        dialect="postgresql",
        config=SQLConfig(strict_mode=True)
    )

    # Real-time security check
    if not validation.is_safe:
        raise HTTPException(
            status_code=400,
            detail={
                "error": "Query validation failed",
                "issues": validation.issues,
                "risk_level": validation.risk_level.name
            }
        )

    # Extract real-time analysis
    analysis = api_processor._analysis_cache.get(expression.sql())

    # Execute with driver
    async with driver_session() as driver:
        result = await driver.execute(processed_expr)

        return {
            "data": result.rows,
            "metadata": {
                "row_count": result.row_count,
                "analysis": {
                    "complexity_score": analysis.metrics.get("complexity_score", 0),
                    "tables_accessed": analysis.metrics.get("tables", []),
                    "join_count": analysis.metrics.get("join_count", 0),
                }
            }
        }
```

### 3. Middleware Integration

```python
class SQLProcessingMiddleware:
    """Real-time SQL processing middleware."""

    def __init__(self):
        self.processor = UnifiedProcessor(
            transformers=[CommentRemover(), ParameterizeLiterals()],
            validators=[PreventDDL(), SuspiciousKeywords(), TautologyConditions()],
            analyzers=[StatementAnalyzer()],
            cache_analysis=True,
        )

    async def process_request(self, sql: str, context: dict) -> dict:
        """Process SQL request in real-time."""

        # Real-time processing
        expression = sqlglot.parse_one(sql)
        processed_expr, validation = self.processor.process(
            expression,
            dialect=context.get("dialect", "postgresql"),
            config=SQLConfig(strict_mode=context.get("strict_mode", True))
        )

        # Real-time analysis extraction
        analysis = self.processor._analysis_cache.get(expression.sql())

        return {
            "processed_sql": processed_expr.sql(),
            "is_safe": validation.is_safe,
            "security_issues": validation.issues,
            "warnings": validation.warnings,
            "real_time_analysis": {
                "complexity_score": analysis.metrics.get("complexity_score", 0),
                "security_risk": validation.risk_level.name,
                "performance_indicators": {
                    "join_count": analysis.metrics.get("join_count", 0),
                    "subquery_count": analysis.metrics.get("subquery_count", 0),
                    "function_count": analysis.metrics.get("function_count", 0),
                },
                "structural_analysis": {
                    "statement_type": analysis.metrics.get("statement_type"),
                    "tables_accessed": analysis.metrics.get("tables", []),
                    "has_aggregates": analysis.metrics.get("has_aggregates", False),
                    "has_subqueries": analysis.metrics.get("has_subqueries", False),
                }
            }
        }
```

**REAL-TIME MONITORING AND OBSERVABILITY**:

### Pipeline Metrics and Insights

```python
# Real-time pipeline monitoring
class PipelineMonitor:
    """Monitor unified processor performance and insights."""

    def __init__(self, processor: UnifiedProcessor):
        self.processor = processor
        self.metrics = {
            "queries_processed": 0,
            "security_blocks": 0,
            "performance_warnings": 0,
            "cache_hits": 0,
            "processing_times": [],
        }

    def get_real_time_stats(self) -> dict:
        """Get real-time processing statistics."""

        cache_size = len(self.processor._analysis_cache)

        return {
            "pipeline_health": {
                "queries_processed": self.metrics["queries_processed"],
                "cache_efficiency": {
                    "size": cache_size,
                    "hit_ratio": self.metrics["cache_hits"] / max(self.metrics["queries_processed"], 1),
                },
                "security_metrics": {
                    "blocks": self.metrics["security_blocks"],
                    "block_rate": self.metrics["security_blocks"] / max(self.metrics["queries_processed"], 1),
                },
                "performance_metrics": {
                    "warnings": self.metrics["performance_warnings"],
                    "avg_processing_time": sum(self.metrics["processing_times"]) / max(len(self.metrics["processing_times"]), 1),
                }
            },
            "real_time_insights": {
                "most_complex_queries": self._get_complex_queries(),
                "common_security_issues": self._get_security_patterns(),
                "performance_bottlenecks": self._get_performance_issues(),
            }
        }
```

**KEY POINTS FOR DOCS**:

- **Real-Time Processing**: Single-pass analysis, transformation, and validation with minimal overhead
- **Comprehensive Security**: Multi-layered threat detection including injection, DDL prevention, and suspicious patterns
- **Performance Intelligence**: Real-time complexity analysis, join optimization, and cartesian product detection
- **Audit and Compliance**: Automatic audit trails, compliance checking, and regulatory requirement enforcement
- **Shared Analysis**: Unified analysis engine eliminates redundant processing across all components
- **Extensible Architecture**: ProcessorProtocol enables custom validators, transformers, and analyzers
- **Production Ready**: Intelligent caching, minimal overhead, and comprehensive error handling
- **Developer Experience**: Rich real-time insights, type-safe interfaces, and comprehensive observability
- **Integration Friendly**: Seamless integration with driver protocol, APIs, middleware, and monitoring systems
- **Zero-Compromise Security**: Real-time threat detection without sacrificing performance or functionality

---

## üìù TEMPLATE FOR NEW ENTRIES

Copy this template when adding new reference sections:

```markdown
## [REF-XXX] Title: Brief Description

**DECISION**: What was decided and why.

**IMPLEMENTATION**:
- Key implementation details
- Important code patterns
- Architectural choices

**USER BENEFIT**:
- How this helps users
- What problems it solves
- Performance/DX improvements

**CODE EXAMPLES**:
```python
# Show user-facing examples
# Include both basic and advanced usage
```

**KEY POINTS FOR DOCS**:

- Important concepts to emphasize
- Common pitfalls to avoid
- Integration considerations

---

**üìã NEXT SECTIONS TO ADD**:

- Error handling patterns and custom exceptions
- Arrow integration architecture
- Statement builder integration
- Extension system design
- Testing patterns and fixtures
- Performance optimization techniques
- Database-specific adapter patterns
